#   abind(X_train_mask_4, along = 1) %>%
#   abind(X_train_mask_5, along = 1)
# dimnames(X_train_mask) <- NULL
# rm(X_train_mask_1, X_train_mask_2, X_train_mask_3, X_train_mask_4, X_train_mask_5,
#    X_train_raw_1, X_train_raw_2, X_train_raw_3, X_train_raw_4, X_train_raw_5)
# save(X_train_mask, X_train_raw, file = "./data_processed/Train.RData")
load("./data_processed/TrainData.RDA")
# Loss function -----------------------------------------------------
dice_coef <- function(y_true, y_pred, smooth = 1.0) {
y_true_f <- k_flatten(y_true)
y_pred_f <- k_flatten(y_pred)
intersection <- k_sum(y_true_f * y_pred_f)
result <- (2 * intersection + smooth) /
(k_sum(y_true_f) + k_sum(y_pred_f) + smooth)
return(result)
}
bce_dice_loss <- function(y_true, y_pred) {
result <- loss_binary_crossentropy(y_true, y_pred) +
(1 - dice_coef(y_true, y_pred))
return(result)
}
# U-net 128 -----------------------------------------------------
get_unet_128 <- function(input_shape = c(128, 128, 3),
num_classes = 1) {
inputs <- layer_input(shape = input_shape)
# 128
down1 <- inputs %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down1_pool <- down1 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 64
down2 <- down1_pool %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down2_pool <- down2 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 32
down3 <- down2_pool %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down3_pool <- down3 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 16
down4 <- down3_pool %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down4_pool <- down4 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 8
center <- down4_pool %>%
layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# center
up4 <- center %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down4, .), axis = 3)} %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 16
up3 <- up4 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down3, .), axis = 3)} %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 32
up2 <- up3 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down2, .), axis = 3)} %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 64
up1 <- up2 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down1, .), axis = 3)} %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 128
classify <- layer_conv_2d(up1,
filters = num_classes,
kernel_size = c(1, 1),
activation = "sigmoid")
model <- keras_model(
inputs = inputs,
outputs = classify
)
model %>% compile(
optimizer = optimizer_rmsprop(lr = 0.0001),
loss = bce_dice_loss,
metrics = custom_metric("dice_coef", dice_coef)
)
return(model)
}
sem_seg_model <- get_unet_128()
#
# sem_seg_model %>%
#   fit(x = X_train_raw, y = X_train_mask,
#       epochs = 5
#       )
file_path_model <- "./models/SemSeg.H5"
# save_model_hdf5(sem_seg_model, filepath = file_path_model)
load_model_weights_hdf5(object = sem_seg_model, filepath = file_path_model)
sem_seg_model <- get_unet_128()
sem_seg_model %>%
fit(x = X_train_raw, y = X_train_mask,
epochs = 5
)
knitr::opts_chunk$set(echo = T, warning = F, message = F)
# Data Manipulation
library(dplyr)
library(tidyr)
library(abind)
# Deep Learning
library(keras)
# Image Processing / Visualisation
library(jpeg)
library(OpenImageR)
library(imager)
library(ggplot2)
library(tiff)
train_images <- list.files(path = "./data_raw_SemanticSegmentation/train/images/",
recursive = T)
validation_files <- list.files(path = "./data_raw_SemanticSegmentation/train/gt/",
recursive = T)
image_file <- paste0("./data_raw_SemanticSegmentation/train/images/", train_images[1])
image_tif <- tiff::readTIFF(image_file)
dim(image_tif)
OpenImageR::imageShow(image_tif)
sample_image <- function(image_file) {
# samples of 512x512 are taken from raw and mask
# resized to 128x128
# returned as arrays
image_mask <- tiff::readTIFF(paste0("./data_raw_SemanticSegmentation/train/gt/", image_file))
image_raw <- tiff::readTIFF(paste0("./data_raw_SemanticSegmentation/train/images/", image_file))
dims <- image_raw %>% dim
resolution <- 256
x <- runif(n = 1,
min = 1,
max = dims[1] - resolution)
y <- runif(n = 1,
min = 1,
max = dims[1] - resolution)
sample_image_raw <- image_raw[x:(x+resolution-1), y:(y+resolution-1), ] %>%
resizeImage(width = 128, height = 128)
sample_image_mask <- image_mask[x:(x+resolution-1), y:(y+resolution-1)] %>%
resizeImage(width = 128, height = 128)
list(sample_image_raw, sample_image_mask)
}
c(sample_raw, sample_mask) %<-% sample_image(train_images[1])
OpenImageR::imageShow(sample_raw)
OpenImageR::imageShow(sample_mask)
data_preparation <- function(images, n_obs) {
# prepare the data arrays
X_train_raw <- array(NA, dim = c(n_obs, 128, 128, 3))
X_train_mask <- array(NA, dim = c(n_obs, 128, 128, 1))
# populate the arrays
for (i in 1:n_obs) {
c(sample_raw, sample_mask) %<-% sample_image(images[i])
X_train_raw[i,,, ] <- sample_raw
X_train_mask[i,,, ] <- sample_mask
print(i)
}
list(X_train_raw, X_train_mask)
}
# create training data by sampling images from the original data
# c(X_train_raw_1, X_train_mask_1) %<-% data_preparation(images = train_images,
#                                                    n_obs = 180)
# c(X_train_raw_2, X_train_mask_2) %<-% data_preparation(images = train_images,
#                                                    n_obs = 180)
# c(X_train_raw_3, X_train_mask_3) %<-% data_preparation(images = train_images,
#                                                    n_obs = 180)
# c(X_train_raw_4, X_train_mask_4) %<-% data_preparation(images = train_images,
#                                                    n_obs = 180)
# c(X_train_raw_5, X_train_mask_5) %<-% data_preparation(images = train_images,
#                                                    n_obs = 180)
# X_train_raw <- X_train_raw_1 %>%
#   abind(X_train_raw_2, along = 1) %>%
#   abind(X_train_raw_3, along = 1) %>%
#   abind(X_train_raw_4, along = 1) %>%
#   abind(X_train_raw_5, along = 1)
# dimnames(X_train_raw) <- NULL
# X_train_mask <- X_train_mask_1 %>%
#   abind(X_train_mask_2, along = 1) %>%
#   abind(X_train_mask_3, along = 1) %>%
#   abind(X_train_mask_4, along = 1) %>%
#   abind(X_train_mask_5, along = 1)
# dimnames(X_train_mask) <- NULL
# rm(X_train_mask_1, X_train_mask_2, X_train_mask_3, X_train_mask_4, X_train_mask_5,
#    X_train_raw_1, X_train_raw_2, X_train_raw_3, X_train_raw_4, X_train_raw_5)
# save(X_train_mask, X_train_raw, file = "./data_processed/Train.RData")
load("./data_processed/TrainData.RDA")
# Loss function -----------------------------------------------------
dice_coef <- function(y_true, y_pred, smooth = 1.0) {
y_true_f <- k_flatten(y_true)
y_pred_f <- k_flatten(y_pred)
intersection <- k_sum(y_true_f * y_pred_f)
result <- (2 * intersection + smooth) /
(k_sum(y_true_f) + k_sum(y_pred_f) + smooth)
return(result)
}
bce_dice_loss <- function(y_true, y_pred) {
result <- loss_binary_crossentropy(y_true, y_pred) +
(1 - dice_coef(y_true, y_pred))
return(result)
}
# U-net 128 -----------------------------------------------------
get_unet_128 <- function(input_shape = c(128, 128, 3),
num_classes = 1) {
inputs <- layer_input(shape = input_shape)
# 128
down1 <- inputs %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down1_pool <- down1 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 64
down2 <- down1_pool %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down2_pool <- down2 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 32
down3 <- down2_pool %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down3_pool <- down3 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 16
down4 <- down3_pool %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
down4_pool <- down4 %>%
layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))
# 8
center <- down4_pool %>%
layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# center
up4 <- center %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down4, .), axis = 3)} %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 16
up3 <- up4 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down3, .), axis = 3)} %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 32
up2 <- up3 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down2, .), axis = 3)} %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 64
up1 <- up2 %>%
layer_upsampling_2d(size = c(2, 2)) %>%
{layer_concatenate(inputs = list(down1, .), axis = 3)} %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu") %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = "same") %>%
layer_batch_normalization() %>%
layer_activation("relu")
# 128
classify <- layer_conv_2d(up1,
filters = num_classes,
kernel_size = c(1, 1),
activation = "sigmoid")
model <- keras_model(
inputs = inputs,
outputs = classify
)
model %>% compile(
optimizer = optimizer_rmsprop(lr = 0.0001),
loss = bce_dice_loss,
metrics = custom_metric("dice_coef", dice_coef)
)
return(model)
}
sem_seg_model <- get_unet_128()
sem_seg_model %>%
fit(x = X_train_raw, y = X_train_mask,
epochs = 5
)
set.seed(123)
c(sample_raw, sample_mask) %<-% sample_image(train_images[100])
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(pheatmap)
library(keras)
# prepare resulting dataframe
# file_path <- "./data_CNN_exercise/snakeeyes_test.csv"  # set file for test
# snake_eyes <- read.table(file = file_path)  # read test file
# snake_eyes <- snake_eyes[0, ]  # remove all lines, just keep structure
#
# files <- list.files(path = "./data_CNN_exercise")
# files <- files[1]  # only use the first file
#
# for (file in files) {
#   temp <- read.table(file = paste0("./data_CNN_exercise/",file))
#   snake_eyes <- base::rbind(snake_eyes, temp)
#   print(file)
# }
# Clean-Up
# rm(file, file_path, files)
# Import test data
# snake_eyes_test <- read.table("./data_CNN_exercise/snakeeyes_test.csv")
# save results
# save(snake_eyes, snake_eyes_test, file = "SnakeEyes.RDA")
load("SnakeEyes.RDA")
# prepare resulting dataframe
file_path <- "./data_CNN_exercise/snakeeyes_test.csv"  # set file for test
snake_eyes <- read.table(file = file_path)  # read test file
# prepare resulting dataframe
file_path <- "./data_CNN_exercise/snakeeyes_test.csv"  # set file for test
snake_eyes <- read.table(file = file_path)  # read test file
# prepare resulting dataframe
file_path <- "./data_CNN_exercise/snakeeyes_00.csv"  # set file for test
snake_eyes <- read.table(file = file_path)  # read test file
snake_eyes <- snake_eyes[0, ]  # remove all lines, just keep structure
files <- list.files(path = "./data_CNN_exercise")
files <- files[1]  # only use the first file
for (file in files) {
temp <- read.table(file = paste0("./data_CNN_exercise/",file))
snake_eyes <- base::rbind(snake_eyes, temp)
print(file)
}
# Clean-Up
rm(file, file_path, files)
# Import test data
snake_eyes_test <- read.table("./data_CNN_exercise/snakeeyes_00.csv")
# save results
save(snake_eyes, snake_eyes_test, file = "SnakeEyes.RDA")
load("SnakeEyes.RDA")
snake_eyes <- snake_eyes[1:10000, ]
X_train <- snake_eyes[, 2:401]
y_train <- snake_eyes[, 1] - 1
X_test <- snake_eyes_test[, 2:401]
y_test <- snake_eyes_test[, 1]-1
y_train %>% table()
y_test %>% table()
summary(X_train$V28)
X_train <- X_train / 255
X_test <- X_test / 255
summary(X_train$V28)
image_nr <- 20
sample_image <- X_train[image_nr, ] %>%
as.numeric() %>%
matrix(data = ., nrow = 20, ncol = 20)
y_train[image_nr]
pheatmap(sample_image, cluster_rows = F, cluster_cols = F)
n_images <- nrow(X_train)
n_max <- 20
# Training Data
X_train_reshaped <- array(dim = c(n_images, n_max, n_max, 1))
for (i in 1:n_images) {
temp <- X_train[i, ] %>%
as.numeric() %>%
matrix(data = ., nrow=n_max, ncol = n_max)
X_train_reshaped[i, , , 1] <- temp
}
# Test Data
n_images <- nrow(X_test)
X_test_reshaped <- array(dim = c(n_images, n_max, n_max, 1))
for (i in 1:n_images) {
temp <- X_test[i, ] %>%
as.numeric() %>%
matrix(data = ., nrow=n_max, ncol = n_max)
X_test_reshaped[i, , , 1] <- temp
}
create_cnn_model <- function() {
dnn_cnn_model <-
keras_model_sequential() %>%
layer_conv_2d(filters = 2,
kernel_size = 3,
input_shape = c(20, 20, 1),
activation = 'relu') %>%
layer_conv_2d(filters = 8,
kernel_size = c(3, 3),
activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_dropout(rate = 0.4) %>%
layer_conv_2d(filters = 16,
kernel_size = c(3, 3),
activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2,2)) %>%
layer_flatten() %>%
layer_dense(units = 256, activation = 'relu') %>%
layer_dense(units = 64, activation = 'relu') %>%
layer_dense(units = 12, activation = 'softmax') %>%
keras::compile(optimizer = 'adam',
loss = 'sparse_categorical_crossentropy',
metrics = c("acc"))
}
dnn_cnn_model <- create_cnn_model()
history <- dnn_cnn_model %>%
fit(X_train_reshaped, y_train,
validation_split = 0.2,
epochs = 50)
plot(history,
smooth = F)
save_model_hdf5(object = dnn_cnn_model, filepath = "./models/dnn_cnn_snake.h5")
dnn_cnn_model %>% summary()
# prepare resulting dataframe with predicted and actual labels
test_check <- tibble(label_pred = NA,
label_act = y_test %>%
gsub(pattern = "n", replacement = ""))
n_valid <- dim(X_test)[1]
for (id_curr in 1:n_valid) {
test_image <- array(X_test_reshaped[id_curr,,, ], dim = c(1, n_max, n_max, 1))
predictions <- dnn_cnn_model %>%
predict(test_image)
label_pred <-which.max(predictions)-1
test_check$label_pred[id_curr] <- label_pred
}
